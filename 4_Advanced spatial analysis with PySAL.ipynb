{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spatial Analysis with PySAL\n",
    "\n",
    "Before getting into it, let's install [contextily](https://contextily.readthedocs.io/en/latest/) to be able to create base maps from different tile map providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install contextily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, ready to go.\n",
    "\n",
    "**[PySAL](https://pysal.org) is a comprehensive library in Python for spatial data analysis.** It's a project that consolidates different tools and libraries to make spatial analysis in Python easier and more robust. Let's deep dive into some advanced spatial analysis techniques you can perform.\n",
    "\n",
    "## üìñ PySal *viz* Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "import libpysal \n",
    "from libpysal import weights\n",
    "from pysal.explore import esda\n",
    "from pysal.viz import mapclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 10] # change standard figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a [geopackage](http://www.geopackage.org) with districts in [Belo Horizonte](https://en.wikipedia.org/wiki/Belo_Horizonte), reproject it to [EPSG:3857](https://epsg.io/3857), and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = gpd.read_file('https://github.com/darribas/gds_ufmg19/raw/master/data/bh.gpkg').to_crs(epsg=3857)\n",
    "db.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñSeaborn [color palettes](https://seaborn.pydata.org/tutorial/color_palettes.html)\n",
    "\n",
    "Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('viridis', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('coolwarm', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(sns.color_palette('Set2', 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñAssigning Colors to Values: Classification \n",
    "\n",
    "In the raster lecture, we already used a non-linear mapping from data values to colors. There are more elaborate ways to do this, though, particularly if we want to classify the data so that we can reduce the number of color values on the map to preserve readability. We'll go through a few ways to do this (giving you some cartography skills about [Choropleth Maps](https://onlinelibrary-wiley-com.zorac.aub.aau.dk/doi/abs/10.1002/9781118786352.wbieg0951) along the way), starting with **Equal intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.EqualInterval(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make ourselves a little function that will nicely show where the boundaries between the classes are when we want to compare the different classification methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClassification(classi):\n",
    "    # Set up the figure\n",
    "    f, ax = plt.subplots(1, figsize=(9, 6))\n",
    "    # Plot the kernel density estimation (KDE)\n",
    "    sns.kdeplot(db['Average Monthly Wage'], fill=True)\n",
    "    # Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "    sns.rugplot(db['Average Monthly Wage'], alpha=0.5)\n",
    "    # Loop over each break point and plot a vertical red line\n",
    "    for cut in classi.bins:\n",
    "        plt.axvline(cut, color='red', linewidth=0.75)\n",
    "    # Title\n",
    "    ax.set_title(classi.name)\n",
    "    # Display image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantiles, the equal intervals method splits up the data into, *ahem*, equal intervals. Quantiles instead arranges the class boundaries so that we have the same number of data points in each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.Quantiles(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Fisher-Jenks](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)** is a data clustering method designed to determine the \"best\" arrangement of values, such that natural breaks are identified and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.FisherJenks(db['Average Monthly Wage'], k=7)\n",
    "classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClassification(classi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boxplot** should sound familiar (if not, go back to yesterday's notebook). This method will use the same breaks use for the boxplot to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classi = mapclassify.BoxPlot(db['Average Monthly Wage'])\n",
    "classi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the classification relate to the box plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, axs = plt.subplots(2, figsize=(10, 10), gridspec_kw = {'height_ratios':[3, 1]})\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(db['Average Monthly Wage'], fill=True, ax=axs[0])\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(db['Average Monthly Wage'], alpha=0.5, ax=axs[0])\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in classi.bins:\n",
    "    axs[0].axvline(cut, color='red', linewidth=0.75)\n",
    "# Box-Plot\n",
    "axs[1].boxplot(db['Average Monthly Wage'], vert=False)\n",
    "# Set X axis manually\n",
    "axs[1].set_xlim(axs[0].get_xlim())\n",
    "# Title\n",
    "axs[0].set_title(classi.name)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the classification methods to assign colors to features on the map. Which of the following maps is the best? Which one is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classification in ['equal_interval', 'quantiles', 'fisher_jenks']:\n",
    "    f, ax = plt.subplots(1, figsize=(14, 14))\n",
    "    db.plot(column='Average Monthly Wage', scheme=classification, ax=ax, legend=True)\n",
    "    ctx.add_basemap(ax, url=ctx.providers.CartoDB.Voyager)\n",
    "    ax.set_axis_off()\n",
    "    plt.axis('equal')\n",
    "    plt.title(classification)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**equal_interval** classification might yield classes that don't capture the variations effectively. Some regions on the map could appear similar in color even if their values are different.\n",
    "\n",
    "**quantiles** classification ensures an even distribution of data points across the classes. This method highlights areas that are particularly high or low compared to others, making it easier to distinguish relative differences across regions.\n",
    "\n",
    "**fisher_jenks** tries to group similar data points while maximizing differences between groups. This method possibly ofers a more intuitive representation here.\n",
    "\n",
    "In Conclusion while the equal_interval might not fully capture the nuances due to the data's spread, the quantiles method would show relative rankings effectively. The fisher_jenks could be most insightful if there are clear groupings or patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñSpatial Weights \n",
    "[Spatial Weights](http://darribas.org/gds_scipy16/ipynb_md/03_spatial_weights.html) are central components of many areas of spatial analysis. In general terms, for a spatial data set composed of *n* locations (points, areal units, network edges, etc.), the spatial weights matrix expresses the potential for interaction between observations at each pair *i,j* of locations. There is a rich variety of ways to specify the structure of these weights, and PySAL supports the creation, manipulation and analysis of spatial weights matrices across three different general types:\n",
    "\n",
    "- Contiguity Based Weights\n",
    "- Distance Based Weights\n",
    "- Kernel Weights\n",
    "\n",
    "Let's take a look at a lattice example with fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a regular 3x3 lattice and draw it here\n",
    "w = weights.lat2W(3, 3, rook=True)\n",
    "# Get points in a grid\n",
    "l = np.arange(3)\n",
    "xs, ys = np.meshgrid(l, l)\n",
    "# Set up store\n",
    "polys = []\n",
    "# Generate polygons\n",
    "for x, y in zip(xs.flatten(), ys.flatten()):\n",
    "    poly = Polygon([(x, y), (x+1, y), (x+1, y+1), (x, y+1)])\n",
    "    polys.append(poly)\n",
    "# Convert to GeoSeries\n",
    "polys = gpd.GeoSeries(polys)\n",
    "gdf = gpd.GeoDataFrame({'geometry': polys, \n",
    "                        'id': ['P-%s'%str(i).zfill(2) for i in range(len(polys))]})\n",
    "w.remap_ids(gdf['id'].values)\n",
    "# Annotate & Visualise\n",
    "ax = polys.plot(edgecolor='k', facecolor='w')\n",
    "[plt.text(x, y, t, \n",
    "          verticalalignment='center',\n",
    "          horizontalalignment='center') for x, y, t in zip(\n",
    "         [p.centroid.x for p in polys],\n",
    "         [p.centroid.y for p in polys],\n",
    "         [i for i in gdf['id']])]\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate a contiguity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(w.full()[0], \n",
    "             index=gdf['id'],\n",
    "             columns=gdf['id'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñReal-world Data: Belo Horizonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_queen = weights.Queen.from_dataframe(db)\n",
    "pd.DataFrame(w_queen.full()[0], \n",
    "             index=db['CD_GEOCMU'],\n",
    "             columns=db['CD_GEOCMU'],\n",
    "            ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚û°Ô∏è Rook's case vs. Queen's case: \n",
    "\n",
    "- In Rook's case, only sees shared **edges** lead to a connection\n",
    "- In Queen's case, also shared **vertices** (points) lead to a connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñSpatial Autocorrelation\n",
    "\n",
    "For more methods to analyse spatial autocorrelation, check out https://geographicdata.science/book/notebooks/06_spatial_autocorrelation.html\n",
    "\n",
    "We'll just do a quick analysis using **[Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)**\n",
    "\n",
    "We want to analyse the spatial autocorrelation of the industry diversity in Belo Horizonte. For comparison, let's create fake data with random industry diversity data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "db['Random Industry Diversity'] = db['Industry Diversity'].sample(frac=1).values\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran = esda.Moran(db['Industry Diversity'], w_queen)\n",
    "print(moran.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moran.p_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moran_shuffled = esda.Moran(db['Random Industry Diversity'], w_queen)\n",
    "print(moran_shuffled.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moran_shuffled.p_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úèÔ∏èTask:\n",
    "\n",
    "Write some code to:\n",
    "\n",
    "1. Download the countries dataset from https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/\n",
    "2. Make a choropleth map of the population density (column `POP_EST` divided by country area)\n",
    "3. Calculate Moran's I to understand whether there is spatial autocorrelation of the gross domestic product per capita (`GDP_MD` / `POP_EST`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
